Current Plan:

We will have 2 servers.  The first, called the capture server, will have a Napatech card and a Mellanox 100g card.  The second, called the compute or storage server, will have a 100g Mellanox card and 8 SSDs.  The idea is that we capture packets on the capture server off of the Napatech card and forward them over the 100g mellanox card to the storage server.  The storage server will have 8 threads of a version of the stardust wdcap service writing, one to each of the SSDs.

Progress so far:

WAND's libtrace package has a tool called tracemcast.  We have this built with Napatech's DPDK library and can forward the packets down the 100G Mellanox link over multicast UDP without problem.  I'm still in the process of modifying our existing code to capture those packets to disk, and then measuring loss.

Concerns I have:

Recently, the stardust collection process that this is loosely based on has reached a limit in its ability to collect telescope data.  And this is on a 10g link, where traffic went from 1gbps to 1.5gbps.  This causes me concern that our model is overly complicated.  Admittedly, on the telescope/stardust project, we do full packet captures and on this 100G project, we only want header captures.  Also, the bottleneck on the telescope collectors is mostly post capture processing.  The actual ability to capture packets appears to be unaffected.  But, it still gives me concern.

We also will need to modify tracemcast to only forward headers instead of full packets.  This shouldn't be too difficult.  I'm more concerned about effectively doing 100gbit capture on both the capture server and the storage server.  Our testing so far in the lab has at least indicated that the mellanox cards are able to at least do reasonably well as capture devices, but are inferior to a dedicated card like the napatech.  So this begs the question.. should we simplify the process and move the card to the storage server and just capture straight to disk?

Questions for 2022-02-08 Meeting

As our possible first test case, what does the link(s) at DREN look like?  Overall bandwidth vs packets per second?  I can then work to simulate that in the lab as a test.

To the folks outside CAIDA... what kinds of features in a 100g monitor would be of value?  The MIT/LL stats?  Other kinds of stats?

Unrelated to 100g, but to MIT/LL folks.  KC mentioned you'd like to have a link to the live network telescope traffic for ongoing statistics.  Let's discuss if the low hanging fruit of giving you a stardust VM would suffice.
